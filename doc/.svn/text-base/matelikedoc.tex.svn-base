\documentclass{article}
\usepackage{setspace,amsmath,amsfonts,listings,url}
\usepackage{natbib}

\author{John Zedlewski (jzedlewski@hbs.edu)}
\title{Practical Empirical Likelihood Estimation with matElike}

% Handy macros
\newcommand{\melike}{\texttt{matElike}\,}
\newcommand{\Exb}[1]{\ensuremath{\mathbb{E}\left[#1\right]}}
\newcommand{\sumin}{\ensuremath{\sum_{i=1}^N}}
\newcommand{\oneover}[1]{\ensuremath{\frac{1}{#1}}}

\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmax}
\DeclareMathOperator{\var}{var}

\newcommand{\Ddx}[1]{\ensuremath{\frac{\partial}{\partial #1}}}
\newcommand{\Ddxx}[1]{\ensuremath{\frac{\partial^2}{\partial^2 #1}}}
\newcommand{\Dydxx}[2]{\ensuremath{\frac{\partial^2 #1}{\partial^2 #2}}}
\newcommand{\Dydx}[2]{\ensuremath{\frac{\partial #1}{\partial #2}}}
\newcommand{\Ddxy}[2]{\ensuremath{\frac{\partial^2}{\partial #1 \, \partial #2}}}
\newcommand{\DAdxy}[3]{\ensuremath{\frac{\partial^2 #1}{\partial #2 \, \partial #3}}}
\newcommand{\half}{\ensuremath{\frac{1}{2}}}
\newcommand{\R}[1]{\ensuremath{\mathbb{R}^{#1}}}


\lstset{language=Matlab,basicstyle=\small,columns=flexible}
%\onehalfspacing
\begin{document}

\maketitle

\section{Introduction}
\label{sec:introduction}

\melike is a Matlab package that simplifies the process of estimating
moment condition models by Empirical Likelihood and related
methods. It supports nonlinear moment conditions, the free
specification of Cressie-Read parameters, and fast estimators for
linear instrumental variables models. \melike uses a direct
optimization approach combined with modern, efficient nonlinear solver
techniques. This approach has proven to be robust and efficient for
large models (with hundreds of thousands of observations) and models
with difficult nonlinear moment conditions.

The \melike package requires only a standard version of Matlab, with
no specialized toolboxes, although it can optionally take advantage of
the external IPOPT solver library if it is available.

The reader in a hurry, who already has some knowledge of EL methods,
should be able to use \melike after reading the example in section
\ref{sec:simple-example} then the sections on installation and basic
usage (sections \ref{sec:installing-melike} and
\ref{sec:basic-melike}).


\section{A simple example}
\label{sec:simple-example}

Later sections will provide more detail on the models and methods used
by \melike, but it may be helpful to start with an example. Consider a
standard linear instrumental variables model, where we have $N$
observations of $x$ (a set of $L$ possibly endogenous structural
variables), $z$ (a set of $J$ exogenous instruments), and $y$ (the
dependent variable). Call the $j$-th instrument $z^{(j)}$. We have $J$
moments with expectation 0, namely:

\begin{equation}
\label{eq:momExample}
\Exb{(y - \beta' \, x) \, z^{(j)}} = 0 \qquad  \text{for $j = 1 \ldots J$}
\end{equation}

So we can define our $J$-th moment function to be:
\begin{align}
  \psi_i^{(j)}(\beta) = (y_i - \beta' x_i) \, z_i^{(j)}
\end{align}



We would like to estimate $\beta$ using empirical likelihood. With
\melike, we start by writing a function that computes our functions
$\psi_i^{(j)}(\beta)$ for a given $\beta$ and $j$, returning a column
vector like $( \psi_1^{(j)}(\beta), \ldots, \psi_N^{(j)}(\beta) )'$. In
this example, assume we have stacked the observations into the
matrices $X$ (dimension $N \times L$) and $Z$ (dimension $N \times J$)
and stored $X$, $y$, and $Z$ in global variables. Then our moment
function is simply Listing \ref{demoMoments}.

\begin{lstlisting}[frame=lines,float,caption={Moment function for
  linear IV},label=demoMoments]
function Mj = demoMoments(theta, elike, j, newTheta)
    global X y Z;
    resid = y - X * theta;
    Mj = Z(:,j) .* resid;   % Interact j-th instrument with residual
end
\end{lstlisting}

Once we've defined the moment function, we configure our model with
\texttt{elSetup} and then pass this configuration to
\texttt{elSolve}, which maximizes the empirical likelihood function
and returns the estimated parameters. Listing \ref{runDemo} shows all
the code needed to estimate the model.


\begin{lstlisting}[frame=lines,float,caption={Solving the linear IV
    model},label=runDemo]
global X y Z;

% ... code to load the data matrices omitted ...

[N L] = size(X);
J = size(Z,2);

elike = elSetup(N, J, L, @demoMoments);  
res = elSolve(elike, 'EL');
disp(res.theta)             % Show the estimated parameters

\end{lstlisting}

And that's it! To run this example or view the complete code, look at
\texttt{matelike/demos/runSimpleDemo.m}. In practice, \melike includes
an optimized function \texttt{elLinearIV} that is faster for models of
this type, but the process for estimating complex nonlinear models is
identical to the example above.

\section{Empirical likelihood theory and implementation}
\label{sec:empirical-likelihood}

Empirical likelihood (EL) provides a method to estimate the parameters
of a model defined by estimating equations like those in equation
\ref{eq:momExample} above, without specifying the distribution of the
disturbances. EL is applicable in many of the settings where the
Generalized Method of Moments (GMM) has been applied, but EL has
attractive properties beyond those of two-step or iterated GMM. In
particular, EL is invariant to the scaling of the data, and
\citet{Newey:2004p495} have shown it to have higher-order bias
properties that are superior to those of GMM. A full discussion of
empirical likelihood theory is beyond the scope of this guide -- in
particular, regularity conditions may be omitted here, and we will
only focus on models actually implemented in \melike. For a better
introduction to the methods themselves, see \citet{Imbens:2002p381} or
the book-length treatment by \citet{Owen:2001p759}.

\subsection{Basic EL model}
\label{sec:basic-el-model}

Throughout this discussion\footnote{This section is loosely based on
  lecture notes from Guido Imbens for Harvard EC2144, Spring 2008}, we
assume that the data are a sample of $N$ iid observations $z_i$. The
empirical log likelihood is defined to be:

\[
L(\pi) = \sum_{i=1}^N \ln \pi_i 
\]

where $\pi$ is a vector of length $N$ such that $0 < \pi_i < 1$
and $\sum_i \pi_i = 1$. Our goal is to estimate $\beta$, a
$K$-dimensional vector whose true value is $\beta_0$. We know that
$\mathbb{E} \psi(z,\beta) = 0$ for $\beta = \beta_0$, but
$\mathbb{E} \psi(z, \beta') \neq 0$ for $\beta' \neq
\beta_0$. Here, $\psi(z,\beta)$ is a vector-valued function of
dimension $J$. We will denote the j-th element of $\psi$ by
$\psi^{(j)}$, as in the example above\footnote{Assume also that
  $\beta \in \beta$, where $\beta$ is a compact subset of
  $\mathbb{R}^K$.}. We estimate $\beta$ by solving:

\begin{equation}
  \label{eq:elmax}
 \hat{\beta} = \argmax_{\pi, \, \beta} \sumin \ln \pi_i  \\
\end{equation}
\begin{equation*}
\text{subject to }
\sumin \pi_i = 1 \, , \quad
\sumin \pi_i \, \psi(z_i, \beta) = 0 \nonumber
\end{equation*}

Clearly, if we did not include the restriction that $\sumin \pi_i \,
\psi(z_i, \beta) = 0$, we would estimate that the unrestricted
$\pi^{unres} = \iota/N$, where $\iota$ is a vector of all 1's. With
this constraint included, our procedure is trying to find the
$\hat{\pi}$ that is ``closest'' to \oneover{N} in a particular sense,
while still satisfying our moment conditions. By using different
notions of ``closeness'' between the estimated $\hat{\pi}$ and
$\iota/N$, we can obtain different estimators. \melike, along with
much of the EL literature, uses the Cressie-Read (CR) class of
functions to express this notion of closeness. The CR family is
parameterized by a scalar $\lambda \in [-2, 1]$. Given two discrete
distributions with the same support over $N$ points and probability
weights $p = (p_1, \ldots, p_N)$ and $q = (q_1, \ldots, q_N)$,
respectively, the CR discrepancy between them is:

\[
I_\lambda(p,q) = \oneover{\lambda \, (1 + \lambda)} \sumin p_i \left[
  (p_i / q_i)^{\lambda} - 1 \right]
\]

This leads to the natural definition of a more general class of
estimators. We will define the estimator:

\begin{equation}
  \label{eq:crmax}
  \hat{\beta}^{CR(\lambda)} = \argmin_{\pi,\beta} I_\lambda(\iota/N,\pi) \quad
  \text{subject to } \sumin \pi_i = 1 \, , \, \sumin \pi_i
  \, \psi(z_i,\beta) = 0
\end{equation}

As a shorthand, we will refer to these parameterized estimators as
$CR(\lambda)$. This class includes the most important Generalized
Empirical Likelihood estimators that have been studied. $CR(0)$ is the
empirical likelihood estimator described above\footnote{For EL and ET,
  we have to interpret this as a limit as $\lambda \rightarrow 0$ or
  $\lambda \rightarrow -1$.}. $CR(-1)$ is the expontial tilting (ET)
estimator studied by \citet{Imbens:1998p513}, and $CR(-2)$ is the
continuously updating estimator (CUE) of
\citet{Hansen:1996p555}. \citet*{Newey:2004p495} have
analyzed the family of Generalized Empirical Likelihood (GEL)
estimators, which are defined as the solution to a saddle point
problem, in detail and shown that all of these $CR(\lambda)$ estimtors
have equivalent GEL versions. As \melike works directly with the
$CR(\lambda)$ formulation, we will not discuss other GEL estimators
here\footnote{The terminology here can get confusing. Newey and Smith
  refer to the $CR(\lambda)$ estimators as MD (minimum discrepancy)
  estimators, but this seems too easy to confuse with minimum distance
  estimation. Note that authors following Newey and Smith often refer
  to a $\gamma$ parmeterizing the GEL-dual estimator. This $\gamma$ is
  {\em not} the same number as the $\lambda$ for CR.}.

Within \melike, changing from an EL estimator to a $CR(\lambda)$
estimator for an arbitrary $\lambda$ simply requires changing a single
function argument, so in the discussion below we will often describe
EL estimation only. Unless otherwise specified, the methods apply
equally well to other $CR(\lambda)$ estimators.

\subsection{EL computation}
\label{sec:el-computation}

The basic formulation of EL leads to a nonlinear programming problem
with $N + K + M$ parameters, which can be a daunting problem for many
optimization methods. For this reason, most approaches to computing EL
estimators have focused on concentrating out some of the
parameters. For instance, optimization can be performed instead over
the $M + K$ space of Lagrange multipliers and structural parameters by
substituting in the value of $\pi_i$ implied by the first order
conditions ($\pi_i = \exp(\mu - 1 + \lambda' \, \psi(z_i, \theta)$,
where $\mu$ is the Lagrange multiplier on the constraint that $\sum_i
\pi_i = 1$). \citet*{Mittelhammer:2003p508} further reduce the dimensionality by
concentrating out the Lagrange multipliers $\lambda$ as well, although
they need to introduce an additional inner layer optimization problem
to perform the concentration. While these
approaches lead to a smaller optimization problem, they can also make
the optimization problem more difficult, leading to frustrating
convergence problems.

The \melike package instead solves equation (\ref{eq:elmax})
directly\footnote{\citet*{Owen:2001p759} describes this ``primal''
  approach to solving EL problems. At the time the book was written,
  leading optimization codes (such as \texttt{nlsol}, which he
  discusses) did not typically support sparse analytic Hessians and
  Jacobians, so they were much less effective for solving large primal
  EL problems.}. The largest block of the Hessian of the Lagrangian is
the $N \times N$ block corresponding to $\Ddxy{\pi}{\pi}
\mathcal{L}$. Off of its main diagonal, this derivative is zero,
however, so the matrix is extremely sparse, with only $N$ nonzero
entries. Optimization codes that exploit a sparse Hessian can solve
such problems efficiently without consuming excessive time or
memory. Convergence is generally quite reliable with this method,
although the scaling of the problem may still be important -- if one
moment condition has values or derivatives several orders of magnitude
greater than another condition, it should be scaled down to ensure
numerical stability.

Versions of Matab before releasee 7.6 (R2008b) do not include solvers
that support sparse Hessians with nonlinear constraints. However,
\melike includes a modern optimization code (\texttt{zipsolver},
developed by John Zedlewski) that does support these features. It also
includes a link to the more robust optimization code IPOPT, developed
by W\"achter and Biegler. IPOPT, written in C++, can be difficult for
the inexperienced user to install, but for large or difficult
problems, it has proven to be more effective than
\texttt{zipsolver}. See the Installation section for more details on
configuring IPOPT. When using Matlab release 7.6 with the optimization
toolbox version 4.0 or greater, \melike can also use the
interior-point algorithm of Matlab's \texttt{fmincon} function. You
can choose which solver to use for each problem by setting the
\texttt{elike.solver} field.

Experience has shown that an exact Hessian and Jacobian are necessary
to obtain fast and reliable convergence for EL problems. Because these
derivatives can be difficult to calculate by hand, \melike includes
code to compute them through automatic differentiation (AD), supported
by the Intlab library (developed by Siegfried Rump, included along
with the default \melike installation). The AD library replaces the
standard Matlab vectors and matrices with special objects that track
all mathematical operations applied to them and compute derivatives
accordingly. In general, this AD process should be transparent to a
user of \melike, but there are a few caveats. Iterative algorithms,
where the number or sequence of iterations depend on $\theta$ also
cannot be used with AD. For moment conditions that are differentiable,
but not compatible with AD, users can code the derivatives directly.
See the \texttt{elSetup} docmentation for more details.

% The basic formulation of EL leads to a nonlinear programming problem
% with $N + K$ parameters. Most nonlinear programming algorithms are
% based on some variation of a Newton's method applied to the first
% order conditions of the problem. So they iterate through a series of
% trial values $\theta_1, \ldots, \theta_k$ for the parameter of
% interest, basing the trial steps on something like: $\theta_{k+1} =
% \theta_{k} - H(\theta_k)^{-1} g(\theta_k)$, where $H$ is the Hessian
% of the objective function and $g$ is the gradient\footnote{Numerical
%   nonlinear programming is an extremely rich and deeply-studied field,
%   and this guide does not have room to go into it in depth. The
%   description here is meant only as a basis for discussion of possible
%   pitfalls. See ``Numerical Optimization (2nd edition)'', by J Nocedal
%   and SJ Wright (2006) for an up-to-date discussion. In particular,
%   chapters 18-19 of the book cover the methods used by
%   \melike.}. Na\"ively computing $H^{-1}$ for an EL problem with
% 100,000 observations would require forming an inverting a matrix
% requiring approximately 80 gigabytes of memory. To avoid this problem,
% the econometric literature has focused on various approaches to
% ``profile out'' some of the parameters and reduce the dimensionality
% of the problem.


\section{Basic  \melike installation and usage}
\label{sec:installing-melike}

Note that all \melike functions support Matlab's online help system,
so you can type \texttt{help elSetup} at the command prompt to view
documentation for the \texttt{elSetup} function, for instance.

\subsection{Installation}

To install \melike, unzip the \texttt{matelike12.zip} archive and
place the resulting \texttt{melroot} directory wherever you like to
store your Matlab code. Add the subdirectory \texttt{melroot/matelike}
to your Matlab path with a command like:

\begin{verbatim}
   addpath c:/matlab/melroot/matelike
\end{verbatim}

where \texttt{c:/matlab} is replaced with the base directory in which
matelike is stored. Edit the file \texttt{matelike/elLoad.m} so that
the variable \texttt{BASEPATH} points to the directory where \melike
is installed. Now the package is installed, but you must load it into
Matlab's memory in each session before you can use it. Load the
package with the command:

\begin{verbatim}
   elLoad
\end{verbatim}

you should see some output and then ``Loaded matElike verson 0.1.4
package successfully''. To avoid repeating these steps every time you
run Matlab, you can add those commands to your 'startup.m' file
(search the Matlab help docs for startup.m for information on how to
do this).

\subsection{Using IPOPT (optional)}

If you want to use IPOPT instead of the default zipsolver, you must
install the IPOPT library and its Matlab interface. See
\url{http://www.coin-or.org/Ipopt/documentation/node9.html} for
details on the basic installation and
\url{https://projects.coin-or.org/Ipopt/wiki/MatlabInterface} for
instructions on installing the Matlab interface. If you are not
comfortable compiling C++ code and working with code libraries, this
may be a frustrating process -- you should ask a system administrator
for help. To load \melike with Ipopt as the default solver, initialize
the package with the commmand \texttt{elLoad(true)}. You may need to
edit the variable IPOPTPATH in elLoad.m to point to the base directory
of the IPOPT installation (the one that contains subdirectories called
\texttt{matlab} and \texttt{lib}). Note that your operating system
needs to know how to find the IPOPT libraries too. On Linux, this
typically means that you have to set the \texttt{LD\_LIBRARY\_PATH}
variable from the shell before starting Matlab, with something like:

\begin{verbatim}
   export LD_LIBRARY_PATH=~/matlab/ipopt/lib
\end{verbatim}

 where '~/matlab/ipopt' is replaced with the path to your Ipopt
directory. Assuming you're running this bash shell, you can add this
line to the '.bashrc' file in your home directory to run the command
every time you log in. In Windows, you should add this directory to
the PATH environment variable.

For many users, the default \texttt{zipsolver} will be perfectly
adequate. However, if you have convergence issues with your model, and
you have checked that it is well-specified and well-scaled, Ipopt will
often prove to be more effective.

\subsection{Basic \melike usage}
\label{sec:basic-melike}

Trying out the basic example in section \ref{sec:simple-example} of
this document will be the best way to get up to speed with
\melike. The directory \texttt{matelike/demos} contains that example
(in \texttt{runSimpleDemo.m}), a dynamic panel data example from
\citet*{Imbens:2002p381} (in \texttt{dynamicPanel.m}) and a
Poisson nonlinear instrumental variables model (in
\texttt{demoPoissinst.m}). The Appendix to this document includes
reference documentation for all signficant \melike functions.

In general, there are only a few steps to using \melike for an
arbitrary model:

\begin{itemize}
\item Write a function (which we will call \texttt{momFunction}) that
  evaluates the moment functions for each observation. The function
  may have its own m-file, or it may be a nested function in the same
  file as the calling code. Nested functions make it simple to share
  data between the setup code and the moment function -- see the
  Matlab documentation for details.

\item Call \texttt{elLoad} to initialize \melike (only needs to be
  done once per Matlab session).

\item Call \texttt{elike = elSetup(nObs, nMom, nTheta, @momFunction)},
  specifying the number of observations, number of moment conditions,
  number of elements of $\theta$, and the moment condition function to
  use. The moment function is passed as a Matlab ``function handle''
  -- simply the name of the function preceeded by the \texttt{@}
  symbol.

\item Call \texttt{res = elSolve(elike, method)} to solve the
  model. You can display the results in more detail by then calling
  \texttt{elModelSumm(res)}.

\end{itemize}

For linear two-stage-least-squares models, the \texttt{elLinearIV}
function is much more efficient than using \texttt{elSolve}
directly. To use \texttt{elLinearIV} you can skip the \texttt{elSetup}
step: just put your instruments (Z) and structural variables (X) in to
matrices and pass them into \texttt{elLinearIV}.


\section{Empirical and simulation results}
\label{sec:sample-results}

Both asymptotic theory and Monte Carlo simulations indicate that
empirical ilkelihood can deliver more accurate results than GMM in
many situations. Below, we consider a dynamic panel data model based
on simulated data and a real empirical example with a simple nonlinear
model.

\subsection{Dynamic panel model}
\label{sec:linear-iv}

Imbens (2002) discusses the following dynamic panel data model with
fixed effects and no covariates. We observe $N$ individuals for $T$
time periods. An observation consists of only the dependent variable
$Y_{it}$, which has both an individual-specific unobservable component
($\eta_i$) and a dependence on the previous value of $Y_i$. The exact
model is:

\[
 Y_{it} = \eta_i + \theta \, Y_{i(t-1)} + \varepsilon_{it}
\]

and our goal is to estimate the scalar $\theta$. Lagged values of $Y$
are not correlated with $\varepsilon_{it}$, but they affect $Y_{it}$
through the dynamic part of the model, so even distance lags can be
used as instruments. So we can generate many moment conditions of the
form:

\[
  \psi_{1t}(Y, \theta) = 
 \left( \begin{array}{c}
    Y_{i(t-2)} \cdot
    \left( Y_{it} - Y_{i(t-1)} - \theta \, (Y_{i(t-1)} - Y_{i(t-2)}) \right) \\
    Y_{i(t-3)} \cdot \left(
      Y_{it} - Y_{i(t-1)} - \theta \, (Y_{i(t-1)} - Y_{i(t-2)}) \right)\\
    \ldots \\
    Y_{i1} \cdot \left(
      Y_{it} - Y_{i(t-1)} - \theta \, (Y_{i(t-1)} - Y_{i(t-2)}) \right)
  \end{array}
\right)
\]

Using all possible $\psi_{1t}$ conditions, we have $(T - 1) \times (T
- 2) / 2$ moments. If we assume that the data come from a long-run
steady state of the model, we can add $(T-2)$ more moment conditions:
\[
\psi_{2t}(Y, \theta) = (Y_{i(t-1} - Y_{i(t-2)}) \cdot (Y_{it} - \theta \, Y_{i(t-1)})
\]

The model is greatly overidentified, with $(T + 1) \times (T - 2)$
total instruments for a single parameter. For a small $\theta$, the
long lags contain little information and generate weak
instruments. However, with a value of $\theta$ close to $1$, the
distant lags can significantly improve the precision of our estimate.

While this model can be formulated as a linear instrumental variables
problem, given some careful stacking of lags and differences, the code
in \texttt{dynamicPanel.m} uses the generic \melike formulation
of the moment functions to keep the example code clear.

\begin{table}[ht]
  \centering

\begin{tabular}[h]{|l|r|r||r|r||r|r|}
  \hline
  & \multicolumn{2}{c||}{$\theta = 0.50$} & \multicolumn{2}{c||}{$\theta =
    0.90$} &
  \multicolumn{2}{c|}{$\theta = 0.95$} \\ \hline
  & EL & GMM & EL  & GMM & EL & GMM \\ \hline \hline
  Median bias  & -0.0007 & 0.0006  & -0.0002 & -0.0668 & -0.0001 & -0.2562 \\
  Std. dev.    & 0.0093 & 0.0093   &  0.0126 & 0.0704 & 0.0154 & 0.1977 \\
  Root MSE     & 0.0093 & 0.0093   & 0.0126 & 0.1056 & 0.0154 & 0.3717 \\
  Average time & 3.1 s & 0.62 s    & 3.0 s & 0.62 s & 3.1 s & 0.64 s  \\
  \hline
\end{tabular}
  
  \caption{Monte Carlo results for dynamic panel model: $N = 1500$, $T = 12$}
  \label{tab:dpd-res}
\end{table}

Table \ref{tab:dpd-res} presents the results of 100 Monte Carlo
simulations comparing the performance of Empirical Likelihood and
2-step GMM estimators for this model with simulated data. The dataset
contains 1500 individuals with $T=12$ time periods, and results for
three different parameter values $\theta = \{0.5,0.90,0.95\}$ are
presented. Note that the average time of the EL approach would
decrease significantly if the problem were reformulated for use with
\texttt{elLinearIV}\footnote{In 3 of the 100 the simulations with
  $\theta = 0.95$, the EL estimator failed to converge. These
  simulations were dropped from both the GMM and EL results. The GMM
  estimator uses the identity matrix as its first step weighting
  matrix. Estimation times were recorded on a 2.4 ghz AMD Opteron
  processor.}. For $\theta = 0.5$, GMM and EL have nearly-identical
performance. However, as the autoregressive component of $Y$ gets
large, GMM breaks down and delivers unreliable results.

\subsection{Poisson-IV model}
\label{sec:poisson-iv-model}

\citet*{Mullahy:1997p1042} presents a nonlinear instrumental variables
esimator for a Poisson regression model with an omitted variable that
is correlated with the structural variables ($x$) but not with the
instruments ($z$). While the standard Poisson model uses the
conditional mean specification $\Exb{y | x} = \exp( x \, \theta )$,
Mullahy's model is:

\begin{align*}
  \Exb{ y \, | \, x, \eta } &= \exp( x \beta + \nu )  \\
  & = \exp( x \beta ) \cdot \eta \\
  \Exb{ \eta \,|\, z } &= \tau
\end{align*}

where $\tau$ is normalized to 1. Given $J$ instruments
$\{ z^{(1)},z^{(2)}, \ldots, z^{(J)} \}$, we can consistently estimate
$\beta$ with EL or GMM, using the moment functions:

\[
\psi^{(j)} (\beta) = z^{(j)} \cdot \left( \exp(- x \beta) \, y - 1 \right)
\]

This model is implemented in \texttt{demos/poissinst.m}. Mullahy
estimates a model of cigarette demand, where the $x$ variables include
a measure of ``habit stock,'' which is correlated with the unobserved
$\eta$. As instruments, he includes lagged values of cigarette prices,
lagged values of dummies for various state-wide smoking restrictions,
a cubic term in age and an interaction between age and education,
leading to 15 instruments and 11 structural variables. (See
\texttt{demos/demoPoissinst.m} for more details.) The dataset has 6160
total observations extracted from the 1979 National Health Interview
Survey.

\begin{table}[ht]
  \centering
  \begin{tabular}{|l|rr||rr|r|}
\hline
 & \multicolumn{2}{c|}{EL} & \multicolumn{2}{c|}{GMM} & Rel. diff \\
\hline \hline
Habit stock &    0.0033 & (0.0021) &    0.0040 & (0.0024) &    0.3196 \\ 
Price 79 &   -0.0086 & (0.0044) &   -0.0084 & (0.0044) &    0.0364 \\ 
Rest res. 79 &   -0.0632 & (0.0520) &   -0.0590 & (0.0512) &    0.0806 \\ 
Income &   -0.0062 & (0.0028) &   -0.0070 & (0.0028) &    0.2543 \\ 
Age &    0.0896 & (0.0392) &    0.0776 & (0.0439) &    0.3069 \\ 
$Age^2$ &   -0.0012 & (0.0004) &   -0.0011 & (0.0004) &    0.3090 \\ 
Educ &    0.1412 & (0.0321) &    0.1376 & (0.0313) &    0.1110 \\ 
$Educ^2$ &   -0.0093 & (0.0014) &   -0.0092 & (0.0013) &    0.0949 \\ 
Fam size &   -0.0132 & (0.0125) &   -0.0127 & (0.0125) &    0.0365 \\ 
White &   -0.1026 & (0.0687) &   -0.0903 & (0.0670) &    0.1787 \\ 
Intercept &   -2.5436 & (0.5663) &   -2.4040 & (0.6051) &    0.2464 \\ 
\hline
  \end{tabular}
  \caption{Cigarette demand estimation results (standard error in parentheses)}
  \label{tab:cig-res}
\end{table}

Table \ref{tab:cig-res} presents the results of the model as estimated
by EL and 2-step GMM\footnote{The GMM estimator uses $(Z'Z/N)^{-1}$ as
  its initial weighting matrix. }. The relative difference columm is
the absolute difference between the two estimators, divided by the EL
standard errors. It appears that 2-step GMM has overestimated the
impact of the endogenous habit stock variable by about 30\% of a
standard deviation in this procedure. Note that these GMM results are
slightly different from those presented in Mullahy (1997) -- they
appear to be sensitive to the choice of the initial weighting matrix,
which is a common issue in GMM estimation.

\clearpage
\section{Appendix: \melike function reference}

\newcommand\linesect[1]{\vspace{8pt} \hrule \vspace{8pt} \subsection*{#1}}

This documentation is also available via Matlab's online help
system.

\vspace{20pt}
\lstset{language={},basicstyle=\small,columns=flexible,escapeinside="",aboveskip=1pt}

\input{mellisting}

\section{Appendix: Changes}
\label{changes}
\begin{itemize}
\item 0.1.2 -- first public release
\item 0.1.3 -- added elConfRegion, fixed GMM standard errors, various
  other improvements
\item 0.1.4 -- support for fmincon, documentation improvements
\end{itemize}


\section{References}
\label{sec:references}


\bibliographystyle{plainnat}
\bibliography{elpapers}


\end{document}
